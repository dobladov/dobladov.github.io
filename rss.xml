<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Daniel Doblado</title><description>Personal site of Daniel Doblado</description><link>https://dobla.do/</link><item><title>Automatically deploy to GitHub Pages using Actions</title><link>https://dobla.do/blog/automatically-deploy-to-github-pages-using-actions/</link><guid isPermaLink="true">https://dobla.do/blog/automatically-deploy-to-github-pages-using-actions/</guid><description>How to automate your workflows increasing your productivity and reducing manual errors.</description><pubDate>Fri, 07 Feb 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import actions from &apos;./actions.png&apos;&lt;/p&gt;
&lt;p&gt;&amp;lt;img src={actions.src} alt=&quot;A starry night sky.&quot;/&amp;gt;
This has become the main way to deploy most of my projects and it has greatly reduced the time invested doing it by hand, also avoiding headaches that you might be familiar with like, &quot;ohhh! I forgot to checkout the proper branch&quot;, &quot;I used the wrong script.&quot;, &quot;I accidentally deleted the database and all the backups while updating a CSS color&quot;.&lt;/p&gt;
&lt;p&gt;After setting up this workflow your code will be automatically deployed after each commit to master without human interaction and without having to use other service than GitHub.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“There should be two tasks for a human being to perform to deploy software into a development, test, or production environment: to pick the version and environment and to press the “deploy” button.”
― David Farley, Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Advantages&lt;/h2&gt;
&lt;p&gt;If just mentioning the world &lt;em&gt;&quot;Automatically&quot;&lt;/em&gt; does not catch your attention, these are some of the main reasons why is worth to write some tiny workflows.&lt;/p&gt;
&lt;h3&gt;Single source of truth&lt;/h3&gt;
&lt;p&gt;Your code and and scrips for deployment are on same repository ensuring that all the information to create a build is on the repository and no manual process or tool is missing.&lt;/p&gt;
&lt;h3&gt;Documentation of deployments&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;.yml&lt;/code&gt; workflow shows how to do a proper deployment, other developers should know how it can be replicated locally; Even if there are no comments it might be easy to reverse engineer this files.&lt;/p&gt;
&lt;h3&gt;Automated testing&lt;/h3&gt;
&lt;p&gt;Before each build tests can be enforce to avoid putting code on production that might contain errors or not follow the code style guide.&lt;/p&gt;
&lt;h3&gt;No more manual tasks&lt;/h3&gt;
&lt;p&gt;This is clear, automating a process is always faster and in general less error prone than putting your code manually in production on a Friday before vacations :tada:.&lt;/p&gt;
&lt;h3&gt;Multi-platform&lt;/h3&gt;
&lt;p&gt;Multiple platforms can be tested while doing the deployment without the need of the developers of installing virtual machines.&lt;/p&gt;
&lt;h3&gt;Cost&lt;/h3&gt;
&lt;p&gt;Manual deployment will cost you at the very least time :watch:, GitHub actions are for free both in price and your time, why not use them?&lt;/p&gt;
&lt;h2&gt;Creating a new Action&lt;/h2&gt;
&lt;p&gt;Choose a repository, go to Actions and add new one.&lt;/p&gt;
&lt;p&gt;We need to create a new file, in this case is called &lt;code&gt;deploy.yml&lt;/code&gt;, here we will define the process for the deployment, make sure to &lt;strong&gt;never put any secrets in here&lt;/strong&gt; since it will be committed to the repository.&lt;/p&gt;
&lt;p&gt;The structure goes this way, we need a &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;on&lt;/code&gt; which circumstances we need to run this action, and which &lt;code&gt;jobs&lt;/code&gt; the action will execute.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;name: Deploy to GitHub Pages

on:
  push:
    branches:
      - master

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Build
      uses: actions/setup-node@v3
      with:
        node-version: &apos;20.x&apos;
    - run: |
        npm install
        npm test
        npm run build
        touch dist/.nojekyll
        echo &quot;snoo.odyssey.codes&quot; &amp;gt; dist/CNAME
    - name: Deploy
      uses: JamesIves/github-pages-deploy-action@releases/v3
      with:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        BRANCH: gh-pages
        FOLDER: dist
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After saving this new workflow we should have a file called &lt;code&gt;deploy.yml&lt;/code&gt; inside &lt;code&gt;.github/workflows&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Immediately a new action will be triggered executing the workflow, making a new build and pushing it to the &lt;code&gt;gh-pages&lt;/code&gt; branch.&lt;/p&gt;
&lt;p&gt;After is finish our app should be available at the repository public URL, the URL is show in Setting -&amp;gt; GitHub Pages, here you can also set a custom one.&lt;/p&gt;
&lt;h3&gt;Functionality of each block&lt;/h3&gt;
&lt;p&gt;Let&apos;s go trough each part of the code.&lt;/p&gt;
&lt;p&gt;Here we are indicating push code to master should trigger this Action.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;on:
  push:
    branches:
      - master
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside of jobs we have to define the machine that will run the code, in this case Ubuntu.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;build-and-deploy:
	runs-on: ubuntu-latest
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Steps of the job&lt;/h4&gt;
&lt;p&gt;The Action is now divided in different &lt;code&gt;steps&lt;/code&gt; that will be executed sequentially.&lt;/p&gt;
&lt;p&gt;This step will checkout your repository to make the code available.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- name: Checkout
  uses: actions/checkout@v3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we set node.js and we use version 13 to run the code, I would recommend to read the docs to do some other interesting stuff like running multiple node versions.&lt;/p&gt;
&lt;p&gt;This step will be similar for other code like go or rust&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- name: Build
  uses: actions/setup-node@v3
  with:
    node-version: &apos;20.x&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interesting part, here we have to define the process of actually building the site in this case something similar to what we would execute on our local machine to do it.&lt;/p&gt;
&lt;p&gt;In this case I install the missing node packages, test the code and compile the site.&lt;/p&gt;
&lt;p&gt;After this I added some special stuff for this repository, for example I make sure to create a file indicating that is not a Jekyll repository but this is actually not required in most cases.&lt;/p&gt;
&lt;p&gt;After that I define the CNAME since I want this code to run on a custom domain instead of &lt;code&gt;user.github.io/repository&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- run: |
  npm install
  npm test
  npm run build
  touch dist/.nojekyll
  echo &quot;snoo.odyssey.codes&quot; &amp;gt; dist/CNAME
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this last step we push the code to the &lt;code&gt;gh-pages&lt;/code&gt; branch of the repository&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- name: Deploy
      uses: JamesIves/github-pages-deploy-action@releases/v3
      with:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        BRANCH: gh-pages
        FOLDER: dist
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If any of this steps fails the process will stop ensuring no &quot;bad build&quot; will be out on production.&lt;/p&gt;
&lt;h2&gt;More about it&lt;/h2&gt;
&lt;h3&gt;Committing the build to a different repository&lt;/h3&gt;
&lt;p&gt;I had the use case of having to push the build to a different repository from the one where the sources are.&lt;/p&gt;
&lt;p&gt;In this case I wanted to have the code of my blog on a repository and deploy this build to a GitHubs&apos;s special repository named name.github.io, to serve my personal page from here since &apos;gh-pages&apos; does not work for this repository and my blog does not use Jekyll meaning the source code has to be on a separated repository and the build on master.&lt;/p&gt;
&lt;p&gt;To create this token go to &lt;code&gt;Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Developer settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Personal access tokens&lt;/code&gt; and create a new one.&lt;/p&gt;
&lt;p&gt;With this configuration, we can use a personal token that needs to be set on Settings -&amp;gt; Secrets, since the &lt;code&gt;secrets.GITHUB_TOKEN&lt;/code&gt; only allows you to commit to your current repository.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- name: Deploy
      uses: peaceiris/actions-gh-pages@v2
      env:
        PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }}
        EXTERNAL_REPOSITORY: dobladov/dobladov.github.io
        PUBLISH_BRANCH: master
        PUBLISH_DIR: ./public
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Testing multiple platforms&lt;/h3&gt;
&lt;p&gt;Different platforms might use different commands.&lt;/p&gt;
&lt;p&gt;For example let&apos;s say there&apos;s a &lt;code&gt;rm&lt;/code&gt; in the scripts of the package.js while this build should work on MacOS and Linux, unfortunately it will fail on Windows since the &lt;code&gt;rm&lt;/code&gt; command is not available, with a multiple platform test we will realize that using a solution like &lt;code&gt;rimraf&lt;/code&gt; instead of &lt;code&gt;rm&lt;/code&gt; will solve the problem for Windows.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;strategy:
      matrix:
        platform: [ubuntu-latest, macos-latest, windows-latest]
    runs-on: ${{ matrix.platform }}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Writing a CNAME for custom domains&lt;/h3&gt;
&lt;p&gt;If your deployments needs a custom domain don&apos;t forget to write this file on each build.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;echo &quot;snoo.odyssey.codes&quot; &amp;gt; dist/CNAME
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Running actions on other events&lt;/h3&gt;
&lt;p&gt;Actions are not limited to push to master, is possible to also execute test or do other task on pull request or any kind of supported action, also to schedule this tasks using cron.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;on:
  schedule:
    - cron:  &apos;0 * * * *&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Not only for deployments&lt;/h3&gt;
&lt;p&gt;Actions can be used for creating new release packages, also notifications, emails can be send on each event, the limit is your :rainbow: imagination :rainbow:.&lt;/p&gt;
&lt;h3&gt;Disadvantages&lt;/h3&gt;
&lt;p&gt;One of the main disadvantages, is that like any other service if is down you will not be able of deploying using these workflows, It does not limit you to still do manual deployments for your application.&lt;/p&gt;
&lt;p&gt;If you wonder why your perfectly working action does not work don&apos;t forget to check: https://status.github.com/&lt;/p&gt;
&lt;p&gt;Actions are not a 100% reliable if the infrastructure of GitHub fails, I had a problem where GitHub did not trigger one of my builds when I merged new code, a way to force a new commit without changes is to execute &lt;code&gt;git commit --allow-empty&lt;/code&gt;, this will cause the action to be executed.&lt;/p&gt;
&lt;h2&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Documentation worth reading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/actions/setup-node&quot;&gt;Node Action documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/JamesIves/github-pages-deploy-action&quot;&gt;Deploy Action documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://help.github.com/en/actions/automating-your-workflow-with-github-actions&quot;&gt;GitHub Actions documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/actions/checkout&quot;&gt;Checkout Action documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some examples of my GitHub Actions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/dobladov/snoo/blob/master/.github/workflows/deploy.yml&quot;&gt;Snoo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/dobladov/last-time/blob/master/.github/workflows/deploy.yml&quot;&gt;Last-Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/dobladov/mdx-blog/blob/master/.github/workflows/deploy.yml&quot;&gt;This blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded></item><item><title>Crawling data from dynamically generated sites</title><link>https://dobla.do/blog/crawling-data-from-dynamically-generated-sites/</link><guid isPermaLink="true">https://dobla.do/blog/crawling-data-from-dynamically-generated-sites/</guid><description>How to get data from other sites using free online services</description><pubDate>Fri, 14 Feb 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Let&apos;s start with the need, why I had the necessity to do this,
Recently I created this &lt;a href=&quot;https://observablehq.com/@dobladov/coronavirus-2019-ncov&quot;&gt;Corona Virus Map&lt;/a&gt;, I wanted to be the fastest getting the data from the source site in Chinese, most of the other sites get the data periodical using a cron task and dumping their data locally.&lt;/p&gt;
&lt;p&gt;In my case I wanted to get their data every time the map is loaded, ensuring more accurate results and completely avoiding any manual update of data.&lt;/p&gt;
&lt;h2&gt;Parsing the site&lt;/h2&gt;
&lt;p&gt;So my first try was to curl the page and sadly the results for this &lt;a href=&quot;https://ncov.dxy.cn/ncovh5/view/pneumonia&quot;&gt;site&lt;/a&gt; will not load unless you use Javascript since the tables are dynamically generated.&lt;/p&gt;
&lt;p&gt;This leave us with not many options but to use a browser, so in my case I decided to use Puppeteer, a headless Chrome browser to get the data.&lt;/p&gt;
&lt;p&gt;So after a while playing with it I got the code to do it, it will download and log the data in the format that I need.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const fs = require(&apos;fs&apos;)
const puppeteer = require(&apos;puppeteer&apos;);

(async () =&amp;gt; {
  const browser = await puppeteer.launch()
  const page = await browser.newPage()
  await page.goto(&apos;https://ncov.dxy.cn/ncovh5/view/pneumonia&apos;)
  const data = await page.evaluate(body =&amp;gt; {
    [...document.querySelectorAll(&quot;.close___2yTiY:not(.open___3it7L)&quot;)].forEach(e =&amp;gt; e.click())
    return [...document.querySelectorAll(&apos;.areaBox___3jZkr&apos;)[1].querySelectorAll(&apos;.areaBlock2___27vn7&apos;)].map(row =&amp;gt; {
      const [area, ,confirmed, dead, cured] = [...row.querySelectorAll(&apos;p&apos;)]
      return {
        area: area.innerText,
        confirmed: +confirmed.innerText || 0,
        dead: +dead.innerText || 0,
        cured: +cured.innerText || 0
      }
    })
  })
  console.log(data)
  fs.writeFileSync(`2020-02-16T21:21:26.466Z.json`, JSON.stringify(data, null, 2))
  await browser.close()
})()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After launching this script with &lt;code&gt;node crawler.js&lt;/code&gt; I get a file that I can use as the source data of my visualization.&lt;/p&gt;
&lt;p&gt;All fine but now I have another need, how to get this data every time I load the map?&lt;/p&gt;
&lt;p&gt;For this I definitely need a server running but for this case I wanted to see what can I do with free services.&lt;/p&gt;
&lt;h2&gt;Using Glitch&lt;/h2&gt;
&lt;p&gt;My first approach was to use &lt;a href=&quot;glitch.com&quot;&gt;Glitch&lt;/a&gt;, after setting a express server and a few tries later I got my data every time I curl this url: https://coronacrawler.glitch.me&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const express = require(&quot;express&quot;)
const app = express()
const puppeteer = require(&apos;puppeteer&apos;)

app.use(express.static(&quot;public&quot;))

app.get(&quot;/&quot;, async function(request, response) {
  try {
    const browser = await puppeteer.launch({
      args: [&apos;--no-sandbox&apos;]
    });

    // Same code to get the data before

    res.json(data)
  } catch (error) {
    console.log(error)
  }
});

const listener = app.listen(process.env.PORT, function() {
  console.log(&quot;Your app is listening on port &quot; + listener.address().port)
})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool, but this creates another problem.&lt;/p&gt;
&lt;h3&gt;Cross-origin&lt;/h3&gt;
&lt;p&gt;The map running in ObservableHQ needs to crawl this data and unfortunately if you curl a different domain from a website it needs to have Cross-origin headers enabled or for security reasons it will not load, after adding this to our express server it works.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;app.use(function(req, res, next) {
  res.header(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;)
  res.header(&quot;Access-Control-Allow-Headers&quot;, &quot;Origin, X-Requested-With, Content-Type, Accept&quot;)
  next()
})
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Alternative to Cross-origin&lt;/h4&gt;
&lt;p&gt;Another solution if adding the headers is impossible (For example in a server we can&apos;t control), we can use a service like &lt;a href=&quot;https://cors-anywhere.herokuapp.com/&quot;&gt;cors-anywhere&lt;/a&gt;, just changing the URL to be:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://cors-anywhere.herokuapp.com/https://coronacrawler.glitch.me
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Problems with Glitch&lt;/h4&gt;
&lt;p&gt;Nice, now I have a server running and I can query the data, what else can I ask for? Well while this works there are two main problems, Glitch is more of a place for playing with code and experimenting so it will not be fast.&lt;/p&gt;
&lt;p&gt;Glitch servers go to sleep, it&apos;s perfectly understandable, it makes no sense for Glitch to give computing power for free running all the time, after some period of inactivity the machine will go to sleep and it needs to be awaken the next time, this makes the map unresponsive if there are not many queries and it gives the sensation of being more slow than it is.&lt;/p&gt;
&lt;p&gt;At this point I moved to zeit.co&lt;/p&gt;
&lt;h2&gt;Using Zeit.co&lt;/h2&gt;
&lt;p&gt;Why use zeit.co when Glitch is working?&lt;/p&gt;
&lt;p&gt;This service offers quite a nice free &quot;Hobby&quot; service allowing to use serverless functions.&lt;/p&gt;
&lt;h3&gt;Creating a serverless function&lt;/h3&gt;
&lt;p&gt;We need to modify the previous script a little since we have other constrains.&lt;/p&gt;
&lt;p&gt;First we need a start point for the api query in this case we need a file in &lt;code&gt;/api/index.js&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const getData = require(&apos;./getData&apos;)

module.exports = async (req, res) =&amp;gt; {
  const data = await getData()
  res.json(data)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first problem I encountered is that using puppeteer is a little bit different, luckily this guide &lt;a href=&quot;https://zeit.co/blog/serverless-chrome&quot;&gt;Serverless Chrome via Puppeteer with Now 2.0&lt;/a&gt; explains how to do it well.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const chrome = require(&apos;chrome-aws-lambda&apos;)
const puppeteer = require(&apos;puppeteer-core&apos;)

const getData = async () =&amp;gt; {
  const browser = await puppeteer.launch({
    args: chrome.args,
    executablePath: await chrome.executablePath,
    headless: chrome.headless,
  })

  const page = await browser.newPage()
  await page.goto(&apos;https://ncov.dxy.cn/ncovh5/view/pneumonia&apos;)

// Same code to get the data before

  return data
}

module.exports = getData
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Supporting Cross-origin&lt;/h4&gt;
&lt;p&gt;To enable cors we need a file &lt;code&gt;/api/200.js&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;module.exports = async (req, res) =&amp;gt; {
  res.status(200).send({ message: &apos;cors ok&apos; });
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also our &lt;code&gt;now.js&lt;/code&gt; file should look like this.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &quot;version&quot;: 2,
  &quot;routes&quot;: [
    {
      &quot;src&quot;: &quot;/(.*)&quot;,
      &quot;dest&quot;: &quot;/api/index.js&quot;,
      &quot;headers&quot;: {
        &quot;Access-Control-Allow-Origin&quot;: &quot;*&quot;,
        &quot;Access-Control-Allow-Methods&quot;: &quot;OPTIONS&quot;,
        &quot;Access-Control-Allow-Headers&quot;: &quot;Origin, X-Requested-With, Content-Type, Accept, Authorization&quot;
      }
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this using the command &lt;code&gt;now&lt;/code&gt; should upload the project to zeit.co and the data is available on the project url https://cvirus.dobladov.now.sh/&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;As an alternative to Glitch and Zeit we can use &lt;a href=&quot;https://docs.netlify.com/functions/overview/#manage-your-serverless-functions&quot;&gt;Netlify&lt;/a&gt;, but for this project it was not necessary.&lt;/p&gt;
&lt;p&gt;I&apos;m not affiliated to Glitch of Zeit.co in any way, I just think both are really nice services to run my apps.&lt;/p&gt;
</content:encoded></item><item><title>Fixing a Yamaha DGX-620 grand piano screen</title><link>https://dobla.do/blog/fixing-a-yamaha-dgx-620-screen/</link><guid isPermaLink="true">https://dobla.do/blog/fixing-a-yamaha-dgx-620-screen/</guid><description>How I got my piano and the problems of fixing it</description><pubDate>Sun, 01 Jan 2023 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;import YoutubeVideo from &apos;#astro/components/YoutubeVideo.astro&apos;;&lt;/p&gt;
&lt;h2&gt;Introduction about the piano&lt;/h2&gt;
&lt;p&gt;I bought a second hand &lt;a href=&quot;https://en.wikipedia.org/wiki/Yamaha_DGX-620&quot;&gt;Yamaha DGX-620 Grand Piano&lt;/a&gt; also known as the &amp;lt;i&amp;gt;YPG-625&amp;lt;/i&amp;gt;, the &amp;lt;abbr title=&quot;Yamaha Portable Grand&quot;&amp;gt;YPG&amp;lt;/abbr&amp;gt; stands for Yamaha Portable Grand, sure portable..., after carying it for 10 minutes by myself my back hurt for 3 days.&lt;/p&gt;
&lt;h2&gt;Opinions and problems&lt;/h2&gt;
&lt;p&gt;Overall I&apos;m very happy with it, while I still don&apos;t know much about playing paino, it does look and sounds great, but it came with problem that bothered me a lot, the seller did not mention it until I went to buy it, the screen was broken, it&apos;s a common known manufacturing problem for these models that looking online affects a lot of people, fortunately due to this reason I got a huge discount, only 70€ to buy the full piano, there&apos;s people selling it with a broken screen for around 200 € minimum, the cost new was of ~700€ so a 90% discount.
The problem is very simple, the ribbon connector for he display loses connection overtime, due to a very thin solder that&apos;s in tension.&lt;/p&gt;
&lt;h2&gt;Possible first solution&lt;/h2&gt;
&lt;p&gt;This video shows a very simple solution which seems to work for most who tried, simply add foldback clips to increase the preassure of the connection.&lt;/p&gt;
&lt;p&gt;&amp;lt;YoutubeVideo videoId=&quot;JNsmoBEO4nI&quot; videoTitle=&quot;Yamaha YPG / DGX LCD Screen Fix, Easy &amp;amp; Fast&quot; /&amp;gt;&lt;/p&gt;
&lt;p&gt;This solution sounds great, low repair cost (only 2 foldback clips), without much risk besides open it, so I tried.&lt;/p&gt;
&lt;p&gt;After removing around 100 screws (that was painful), the case can be removed and the board can be accesed.&lt;/p&gt;
&lt;p&gt;{/* &amp;lt;!--  --&amp;gt; */}&lt;/p&gt;
&lt;p&gt;Now that&apos;s open the process it&apos;s easy, after remvoving the ribbons and the 4 screws around the board, the main board will go out and the display can be accessed.
The display has two ribbons that each control half of the screen, I had to fix both in my case, so this requires to remove 2 additional screws to acces the second ribbon.&lt;/p&gt;
&lt;p&gt;{/* &amp;lt;!--  --&amp;gt; */}&lt;/p&gt;
&lt;p&gt;I isolated the clips with masking tape, to make sure the circuits will not short with the metalic clips.&lt;/p&gt;
&lt;p&gt;After this I checked the screen and unfortunately it only fixed half of it, I tried adding more pressure, but nothing worked, so I gave up after a while.&lt;/p&gt;
&lt;p&gt;{/* &amp;lt;!--  --&amp;gt; */}&lt;/p&gt;
&lt;h2&gt;Second option&lt;/h2&gt;
&lt;p&gt;At this poin I thought my only solution was to replace the screen, this was problematic for two reasons, number one, a new screen cost a minimum of &lt;a href=&quot;https://de.aliexpress.com/item/1005004712449602.html&quot;&gt;60€ on aliexprss&lt;/a&gt; sometimes as much as I paid for the whole piano, reson two is that it requires an iron solder, I did want an excuse to buy a &lt;a href=&quot;https://pine64.com/product/pinecil-smart-mini-portable-soldering-iron/&quot;&gt;PINECIL&lt;/a&gt;, an amazing tool, but this will add to the cost, and I decided it was not worth it, besides the piano works great, and the MIDI input can be used without a screen.&lt;/p&gt;
&lt;p&gt;Here it&apos;s when I thought of a differnet solution that so far I didn&apos;t see online, if the solder had desolder why not try to melt it, I do not have a heat gun but I do have a hair dryer so I gave it a try.&lt;/p&gt;
&lt;p&gt;The process is almost the same but instead of adding pins for preassure, I just heated both screen connectors for around 5 minutes using the hair dryer&apos;s max temperature, somtimes adding preassure with the plasticl nozzle.&lt;/p&gt;
&lt;p&gt;After mounting it again I got a gorgeous working screen.&lt;/p&gt;
&lt;p&gt;&amp;lt;YoutubeVideo videoId=&quot;fPGboN25uWo&quot; videoTitle=&quot;Yamaha DGX-620 Grand Piano Screen Repair&quot; /&amp;gt;&lt;/p&gt;
</content:encoded></item><item><title>How to configure a 4k screen in Linux</title><link>https://dobla.do/blog/how-to-configure-a-4k-screen-in-linux/</link><guid isPermaLink="true">https://dobla.do/blog/how-to-configure-a-4k-screen-in-linux/</guid><description>Troubles with your screen?, here is how to fix it.</description><pubDate>Sat, 18 Apr 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;You just got a new cool 4K monitor, you opened it, connected it to your Linux device and the first thing you face is that you can&apos;t select the maximum resolution for the screen.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;My disappointment is immeasurable and my day is ruined. --Reviewbrah&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Why is this problem happening?&lt;/h2&gt;
&lt;p&gt;Apparently some screens might not give the correct &lt;a href=&quot;https://en.wikipedia.org/wiki/Extended_Display_Identification_Data&quot;&gt;EDID&lt;/a&gt; information, in my case a &lt;a href=&quot;https://www.samsung.com/us/business/support/owners/product/ue850-series-u28e850r/&quot;&gt;SAMSUNG LU28E85KRS&lt;/a&gt;, this makes impossible to select all supported modes for our screen, showing some default ones, without including resolutions greater than 1920x1080.&lt;/p&gt;
&lt;h2&gt;How to get the correct EDID and set it using xrandr&lt;/h2&gt;
&lt;h3&gt;Getting the correct EDID&lt;/h3&gt;
&lt;p&gt;Thanks to this wonderful answer &lt;a href=&quot;https://unix.stackexchange.com/a/323121&quot;&gt;How to get EDID for a single monitor?&lt;/a&gt; by &lt;a href=&quot;https://unix.stackexchange.com/users/32896/malat&quot;&gt;Malat&lt;/a&gt;, obtaining the correct information is easy.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt install read-edid
sudo get-edid | parse-edid
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool, we get all supported modes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Modeline    &quot;Mode 1&quot; 148.500 1920 2008 2052 2200 1080 1084 1089 1125 +hsync +vsync
Modeline    &quot;Mode 0&quot; 297.00 3840 4016 4104 4400 2160 2168 2178 2250 +hsync +vsync 
Modeline    &quot;Mode 2&quot; 74.250 1280 1390 1420 1650 720 725 730 750 +hsync +vsync
Modeline    &quot;Mode 3&quot; 148.500 1920 2448 2492 2640 1080 1084 1089 1125 +hsync +vsync
Modeline    &quot;Mode 4&quot; 74.250 1280 1720 1760 1980 720 725 730 750 +hsync +vsync
Modeline    &quot;Mode 5&quot; 27.027 720 736 798 858 480 489 495 525 -hsync -vsync
Modeline    &quot;Mode 6&quot; 27.000 720 732 796 864 576 581 586 625 -hsync -vsync
Modeline    &quot;Mode 7&quot; 74.250 1920 2558 2602 2750 1080 1084 1089 1125 +hsync +vsync
Modeline    &quot;Mode 8&quot; 74.250 1920 2008 2052 2200 1080 1084 1089 1125 +hsync +vsync
Modeline    &quot;Mode 9&quot; 148.50 1920 2008 2052 2200 1080 1084 1089 1125 +hsync +vsync 
Modeline    &quot;Mode 10&quot; 148.50 1920 2448 2492 2640 1080 1084 1089 1125 +hsync +vsync 
Modeline    &quot;Mode 11&quot; 74.25 1280 1390 1430 1650 720 725 730 750 +hsync +vsync 
Modeline    &quot;Mode 12&quot; 241.50 2560 2608 2640 2720 1440 1443 1448 1481 +hsync -vsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Adding the new modes with xrandr&lt;/h3&gt;
&lt;p&gt;The first value after the mode number tell us the horizontal resolution, in my case I&apos;m interested in &lt;code&gt;Mode 0&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Modeline    &quot;Mode 0&quot; 297.00 3840 4016 4104 4400 2160 2168 2178 2250 +hsync +vsync 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&apos;s create a new mode, we can name it anything we want but I prefer to name if after the output resolution.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xrandr --newmode &quot;3840x2160&quot; 297.00 3840 4016 4104 4400 2160 2168 2178 2250 +hsync +
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we add the mode to the outut, if you are not sure which one is the output, run &lt;code&gt;xrandr&lt;/code&gt; and it will show something like &lt;code&gt;HDMI-2 connected&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We use the name of the output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xrandr --addmode HDMI-2 3840x2160
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;:tada: Now the new resolution should be available in the Display settings.&lt;/p&gt;
&lt;p&gt;Don&apos;t forget to set font DPI and screen scale to you taste.&lt;/p&gt;
&lt;h2&gt;Useful information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wiki.archlinux.org/index.php/Xrandr&quot;&gt;Arch wiki xrandr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://gist.github.com/datagrok/410d26e24ec51159cdfd2a400b809705&quot;&gt;Automate setting the screen&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content:encoded></item><item><title>Writing browser extensions without compiling</title><link>https://dobla.do/blog/writing-browser-extensions-without-compiling/</link><guid isPermaLink="true">https://dobla.do/blog/writing-browser-extensions-without-compiling/</guid><description>Positive changes for one of my extensions with the goal of making better extensions.</description><pubDate>Thu, 24 Jun 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Complexity is the enemy&lt;/h2&gt;
&lt;p&gt;Recently one of my most popular &lt;a href=&quot;https://github.com/dobladov/youtube2Anki&quot;&gt;extensions&lt;/a&gt; received a full refactor, previously it was made with just pure JavaScript, this was fine since at the beginning it was a proof of concept, it means no complex bundles, frameworks, compilers, ...&lt;/p&gt;
&lt;p&gt;But with complexity comes necessity, managing the state of the connection between the extension and Anki and editing the sentences got complicated using imperative logic; It&apos;s amazing how used I became to declarative logic with &lt;a href=&quot;https://reactjs.org/&quot;&gt;React&lt;/a&gt; and this is something that I was missing for this extension.&lt;/p&gt;
&lt;p&gt;I could use React, the problem is that if the extension needs to be compiled, the review in the extensions stores will be more exhaustive, taking more time and making longer to fix simple stuff like bugs.&lt;/p&gt;
&lt;p&gt;So in order to give trust and avoid wasting time in reviews, all the code needed to stay just as it was written.&lt;/p&gt;
&lt;p&gt;Tools like &lt;a href=&quot;https://www.snowpack.dev/&quot;&gt;snowpack&lt;/a&gt; made me realize that with the advance of JavaScript modules we might not need to compile what it&apos;s an interpreted language, like seriously how did we reach this point.&lt;/p&gt;
&lt;h3&gt;Coming with simpler solutions&lt;/h3&gt;
&lt;p&gt;Let&apos;s look at the requirements, I wanted to use components, separate my logic from styles, use declarative logic, not set up any bundler (webpack, parcel, etc...) for this React would not make the cut since JSX needs some tooling and care.&lt;/p&gt;
&lt;p&gt;So the decision went to &lt;a href=&quot;https://skruv.io/&quot;&gt;Skruv&lt;/a&gt;, this framework is amazing, not only is tiny, modular and simple to learn, the docs are amazing well organized and clear, covering all functionality without extras.&lt;/p&gt;
&lt;p&gt;You can tell how nice the framework is by looking at this framework&apos;s goals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No build time or runtime dependencies, no parsers&lt;/li&gt;
&lt;li&gt;Pretty small:
&lt;ul&gt;
&lt;li&gt;~350 LOC vDOM&lt;/li&gt;
&lt;li&gt;~100 LOC State management&lt;/li&gt;
&lt;li&gt;~300 LOC HTML/SVG helpers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Useable without bundling/compilation/transpilation&lt;/li&gt;
&lt;li&gt;Fast enough for most normal use cases: benchmark&lt;/li&gt;
&lt;li&gt;Supports async components like import() and async generators&lt;/li&gt;
&lt;li&gt;CSS scoping via shadow DOM&lt;/li&gt;
&lt;li&gt;Hopefully grokable/understandable code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The best part is that to use it I just have to import the files, that&apos;s it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import { div } from  &apos;./skruv/html.js&apos;
import { renderNode } from  &apos;./skruv/vDOM.js&apos;

let root = document.querySelector(&apos;#root&apos;)

root =  renderNode(
	div({}, &apos;Hello world!&apos;),
	root
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On top of those advantages, I love the nested syntax, far shorter than JSX, now that I think about it feels weird that we moved to write HTML in JS.&lt;/p&gt;
&lt;p&gt;You might be also wondering, &quot;what if I need typescript, do you have to transpile?&quot; the answer is no, you can use &lt;a href=&quot;https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html&quot;&gt;JSDocs with Typescript&lt;/a&gt; without having to create a single &lt;code&gt;ts&lt;/code&gt; file.&lt;/p&gt;
&lt;h3&gt;How it ended&lt;/h3&gt;
&lt;p&gt;I feel this change was a success, new code can be iterated easily without having to worry much about state or building, since the extension runs on modern browsers I don&apos;t have to care about babel, etc&lt;/p&gt;
&lt;p&gt;Right now the only concern I have about the extension is manifest v3 which is not supported in Firefox yet, but this is a story for another post.&lt;/p&gt;
&lt;p&gt;I have to say that I&apos;m glad about the feedback people gave, this refactor was motivated because changes on Youtube made it fail and people reported issues with a lot of support, not only that, but I&apos;ve seen articles written in Vietnamese, Japanese and German in how to use the extension and how they find it useful, it really makes want to keep contributing free, anti-traking open-source code.&lt;/p&gt;
</content:encoded></item></channel></rss>